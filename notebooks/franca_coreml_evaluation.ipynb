{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Franca Core ML Model Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation of Franca Core ML models including:\n",
    "- Accuracy comparison with PyTorch reference\n",
    "- Performance benchmarking\n",
    "- Visual analysis of features\n",
    "- Model inspection and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "import coremltools as ct\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Import our evaluation script\n",
    "from scripts.evaluate_coreml import FrancaCoreMLEvaluator\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Loading and Basic Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "COREML_MODEL_PATH = \"../coreml_models/franca_vitb14_in21k_fp32.mlpackage\"\n",
    "MODEL_NAME = \"vitb14\"\n",
    "WEIGHTS = \"IN21K\"\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(COREML_MODEL_PATH):\n",
    "    print(f\"‚ùå Model not found: {COREML_MODEL_PATH}\")\n",
    "    print(\"Run ../scripts/export_coreml_hub.py first to create the model\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model found: {COREML_MODEL_PATH}\")\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = FrancaCoreMLEvaluator(COREML_MODEL_PATH, MODEL_NAME, WEIGHTS)\n",
    "    \n",
    "    # Display model info\n",
    "    spec = evaluator.coreml_model.get_spec()\n",
    "    print(f\"\\nüìã Model Information:\")\n",
    "    print(f\"  Input: {spec.description.input[0].name} - {spec.description.input[0].type}\")\n",
    "    print(f\"  Output: {spec.description.output[0].name} - {spec.description.output[0].type}\")\n",
    "    print(f\"  Model type: {type(evaluator.coreml_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Image Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample image\n",
    "sample_image_path = \"../assets/dog.jpg\"\n",
    "\n",
    "if os.path.exists(sample_image_path):\n",
    "    # Load and display image\n",
    "    image = Image.open(sample_image_path)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(f\"Original Image\\n{image.size}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Resized image (model input)\n",
    "    image_resized = image.resize((518, 518), Image.Resampling.BILINEAR)\n",
    "    axes[1].imshow(image_resized)\n",
    "    axes[1].set_title(f\"Model Input\\n{image_resized.size}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"üîç Evaluating image...\")\n",
    "    result = evaluator.evaluate_single_image(sample_image_path)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìä Evaluation Results:\")\n",
    "    print(f\"  Cosine Similarity: {result['cosine_similarity']:.6f}\")\n",
    "    print(f\"  MSE: {result['mse']:.8f}\")\n",
    "    print(f\"  MAE: {result['mae']:.8f}\")\n",
    "    print(f\"  Max Difference: {result['max_diff']:.8f}\")\n",
    "    print(f\"  PyTorch Time: {result['torch_time']*1000:.1f} ms\")\n",
    "    print(f\"  Core ML Time: {result['coreml_time']*1000:.1f} ms\")\n",
    "    print(f\"  Speedup: {result['speedup']:.2f}x\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Sample image not found: {sample_image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Vector Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'result' in locals():\n",
    "    # Get feature vectors for analysis\n",
    "    image = Image.open(sample_image_path)\n",
    "    \n",
    "    # PyTorch features\n",
    "    torch_input = evaluator.preprocess_image_torch(image)\n",
    "    with torch.no_grad():\n",
    "        torch_features = evaluator.torch_model(torch_input).numpy().flatten()\n",
    "    \n",
    "    # Core ML features\n",
    "    coreml_input = evaluator.preprocess_image_coreml(image)\n",
    "    coreml_result = evaluator.coreml_model.predict({\"image\": coreml_input})\n",
    "    coreml_features = list(coreml_result.values())[0].flatten()\n",
    "    \n",
    "    # Visualize feature distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Feature histograms\n",
    "    axes[0, 0].hist(torch_features, bins=50, alpha=0.7, label='PyTorch', density=True)\n",
    "    axes[0, 0].hist(coreml_features, bins=50, alpha=0.7, label='Core ML', density=True)\n",
    "    axes[0, 0].set_title('Feature Value Distributions')\n",
    "    axes[0, 0].set_xlabel('Feature Value')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Feature comparison scatter\n",
    "    axes[0, 1].scatter(torch_features, coreml_features, alpha=0.6, s=1)\n",
    "    axes[0, 1].plot([torch_features.min(), torch_features.max()], \n",
    "                    [torch_features.min(), torch_features.max()], 'r--', alpha=0.8)\n",
    "    axes[0, 1].set_title('PyTorch vs Core ML Features')\n",
    "    axes[0, 1].set_xlabel('PyTorch Features')\n",
    "    axes[0, 1].set_ylabel('Core ML Features')\n",
    "    \n",
    "    # Feature differences\n",
    "    diff = torch_features - coreml_features\n",
    "    axes[1, 0].hist(diff, bins=50, alpha=0.7)\n",
    "    axes[1, 0].set_title('Feature Differences (PyTorch - Core ML)')\n",
    "    axes[1, 0].set_xlabel('Difference')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Feature magnitudes\n",
    "    indices = np.arange(len(torch_features))\n",
    "    axes[1, 1].plot(indices, np.abs(torch_features), label='PyTorch', alpha=0.7)\n",
    "    axes[1, 1].plot(indices, np.abs(coreml_features), label='Core ML', alpha=0.7)\n",
    "    axes[1, 1].set_title('Feature Magnitudes')\n",
    "    axes[1, 1].set_xlabel('Feature Index')\n",
    "    axes[1, 1].set_ylabel('Absolute Value')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nüìà Feature Statistics:\")\n",
    "    print(f\"  PyTorch - Mean: {torch_features.mean():.4f}, Std: {torch_features.std():.4f}\")\n",
    "    print(f\"  Core ML - Mean: {coreml_features.mean():.4f}, Std: {coreml_features.std():.4f}\")\n",
    "    print(f\"  Difference - Mean: {diff.mean():.6f}, Std: {diff.std():.6f}\")\n",
    "    print(f\"  Correlation: {np.corrcoef(torch_features, coreml_features)[0,1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance benchmark\n",
    "print(\"‚ö° Running performance benchmark...\")\n",
    "benchmark_results = evaluator.benchmark_performance(num_runs=20)\n",
    "\n",
    "# Visualize timing results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Timing comparison\n",
    "torch_times = np.array(benchmark_results['torch_times']) * 1000  # Convert to ms\n",
    "coreml_times = np.array(benchmark_results['coreml_times']) * 1000\n",
    "\n",
    "axes[0].boxplot([torch_times, coreml_times], labels=['PyTorch', 'Core ML'])\n",
    "axes[0].set_title('Inference Time Comparison')\n",
    "axes[0].set_ylabel('Time (ms)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup distribution\n",
    "speedups = torch_times / coreml_times\n",
    "axes[1].hist(speedups, bins=15, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(speedups.mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {speedups.mean():.2f}x')\n",
    "axes[1].set_title('Speedup Distribution')\n",
    "axes[1].set_xlabel('Speedup Factor')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "print(f\"  PyTorch: {benchmark_results['torch_mean_time']*1000:.1f} ¬± {benchmark_results['torch_std_time']*1000:.1f} ms\")\n",
    "print(f\"  Core ML: {benchmark_results['coreml_mean_time']*1000:.1f} ¬± {benchmark_results['coreml_std_time']*1000:.1f} ms\")\n",
    "print(f\"  Average Speedup: {benchmark_results['speedup_mean']:.2f}x\")\n",
    "print(f\"  Speedup Range: {speedups.min():.2f}x - {speedups.max():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Size and Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Analyze model files\n",
    "coreml_path = Path(COREML_MODEL_PATH)\n",
    "if coreml_path.exists():\n",
    "    # Get model size\n",
    "    if coreml_path.is_dir():  # .mlpackage is a directory\n",
    "        total_size = sum(f.stat().st_size for f in coreml_path.rglob('*') if f.is_file())\n",
    "    else:\n",
    "        total_size = coreml_path.stat().st_size\n",
    "    \n",
    "    size_mb = total_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"üìÅ Model File Analysis:\")\n",
    "    print(f\"  Path: {coreml_path}\")\n",
    "    print(f\"  Size: {size_mb:.1f} MB\")\n",
    "    print(f\"  Type: {'Directory' if coreml_path.is_dir() else 'File'}\")\n",
    "    \n",
    "    # Check for other model variants\n",
    "    model_dir = coreml_path.parent\n",
    "    print(f\"\\nüìÇ Available Models in {model_dir}:\")\n",
    "    for model_file in model_dir.glob(\"*.mlpackage\"):\n",
    "        if model_file.is_dir():\n",
    "            file_size = sum(f.stat().st_size for f in model_file.rglob('*') if f.is_file())\n",
    "        else:\n",
    "            file_size = model_file.stat().st_size\n",
    "        print(f\"  {model_file.name}: {file_size/(1024*1024):.1f} MB\")\n",
    "\n",
    "# Memory usage estimation\n",
    "print(f\"\\nüß† Memory Usage Estimation:\")\n",
    "print(f\"  Input tensor (518x518x3 float32): {518*518*3*4/(1024*1024):.1f} MB\")\n",
    "print(f\"  Output tensor (768 float16): {768*2/1024:.1f} KB\")\n",
    "print(f\"  Model parameters: ~{size_mb:.1f} MB\")\n",
    "print(f\"  Estimated peak memory: ~{size_mb + 518*518*3*4/(1024*1024) + 50:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Evaluation (if you have multiple images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: evaluate multiple images if available\n",
    "# You can modify this to point to your own image directory\n",
    "\n",
    "# Check for common image directories\n",
    "possible_dirs = [\"../assets\", \"../test_images\", \"./images\"]\n",
    "image_dir = None\n",
    "\n",
    "for dir_path in possible_dirs:\n",
    "    if os.path.exists(dir_path):\n",
    "        # Check if it contains images\n",
    "        image_files = list(Path(dir_path).glob(\"*.jpg\")) + list(Path(dir_path).glob(\"*.png\"))\n",
    "        if image_files:\n",
    "            image_dir = dir_path\n",
    "            break\n",
    "\n",
    "if image_dir:\n",
    "    print(f\"üìÅ Found images in: {image_dir}\")\n",
    "    \n",
    "    # Evaluate directory (limit to 5 images for demo)\n",
    "    batch_results = evaluator.evaluate_directory(image_dir, max_images=5)\n",
    "    \n",
    "    if batch_results.get('num_images', 0) > 0:\n",
    "        # Plot batch results\n",
    "        individual_results = batch_results['individual_results']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Cosine similarities\n",
    "        cos_sims = [r['cosine_similarity'] for r in individual_results]\n",
    "        axes[0, 0].bar(range(len(cos_sims)), cos_sims)\n",
    "        axes[0, 0].set_title('Cosine Similarities')\n",
    "        axes[0, 0].set_xlabel('Image Index')\n",
    "        axes[0, 0].set_ylabel('Cosine Similarity')\n",
    "        axes[0, 0].set_ylim([0, 1])\n",
    "        \n",
    "        # Inference times\n",
    "        torch_times = [r['torch_time']*1000 for r in individual_results]\n",
    "        coreml_times = [r['coreml_time']*1000 for r in individual_results]\n",
    "        x = np.arange(len(torch_times))\n",
    "        width = 0.35\n",
    "        axes[0, 1].bar(x - width/2, torch_times, width, label='PyTorch')\n",
    "        axes[0, 1].bar(x + width/2, coreml_times, width, label='Core ML')\n",
    "        axes[0, 1].set_title('Inference Times')\n",
    "        axes[0, 1].set_xlabel('Image Index')\n",
    "        axes[0, 1].set_ylabel('Time (ms)')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # MSE values\n",
    "        mse_values = [r['mse'] for r in individual_results]\n",
    "        axes[1, 0].bar(range(len(mse_values)), mse_values)\n",
    "        axes[1, 0].set_title('Mean Squared Errors')\n",
    "        axes[1, 0].set_xlabel('Image Index')\n",
    "        axes[1, 0].set_ylabel('MSE')\n",
    "        \n",
    "        # Speedups\n",
    "        speedups = [r['speedup'] for r in individual_results]\n",
    "        axes[1, 1].bar(range(len(speedups)), speedups)\n",
    "        axes[1, 1].set_title('Speedup Factors')\n",
    "        axes[1, 1].set_xlabel('Image Index')\n",
    "        axes[1, 1].set_ylabel('Speedup (x)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\nüìä Batch Evaluation Summary ({batch_results['num_images']} images):\")\n",
    "        print(f\"  Average Cosine Similarity: {batch_results['cosine_similarity_mean']:.4f} ¬± {batch_results['cosine_similarity_std']:.4f}\")\n",
    "        print(f\"  Average MSE: {batch_results['mse_mean']:.6f} ¬± {batch_results['mse_std']:.6f}\")\n",
    "        print(f\"  Average Speedup: {batch_results['speedup_mean']:.2f}x ¬± {batch_results['speedup_std']:.2f}x\")\n",
    "        \n",
    "else:\n",
    "    print(\"üìÅ No image directories found for batch evaluation\")\n",
    "    print(\"   You can add images to ../assets/ or modify the paths above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "print(\"üìã Generating comprehensive evaluation report...\")\n",
    "report = evaluator.generate_report(\"../evaluation_report.json\")\n",
    "\n",
    "# Display key findings\n",
    "print(f\"\\nüéØ Key Findings:\")\n",
    "perf = report['performance_benchmark']\n",
    "print(f\"  ‚ö° Performance: {perf['speedup_mean']:.2f}x speedup over PyTorch\")\n",
    "print(f\"  üéØ Core ML Time: {perf['coreml_mean_time']*1000:.1f} ¬± {perf['coreml_std_time']*1000:.1f} ms\")\n",
    "\n",
    "if 'sample_image_test' in report:\n",
    "    sample = report['sample_image_test']\n",
    "    print(f\"  üîç Accuracy: {sample['cosine_similarity']:.4f} cosine similarity\")\n",
    "    print(f\"  üìè Precision: {sample['mse']:.6f} MSE\")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete! Report saved to: evaluation_report.json\")\n",
    "print(f\"\\nüöÄ The Core ML model is ready for production use in iOS/macOS apps!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
