{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Franca Core ML Model Format Comparison\n",
    "\n",
    "Compare different Core ML model formats:\n",
    "- **FP32 vs FP16** precision\n",
    "- **.mlpackage vs .mlprogram** formats\n",
    "- **Performance vs Size** trade-offs\n",
    "- **Accuracy preservation** analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import coremltools as ct\n",
    "\n",
    "# Import our comparison script\n",
    "from scripts.compare_model_formats import ModelFormatComparator, create_mlprogram_models\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Current Model Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what models we currently have\n",
    "model_dir = Path(\"../coreml_models\")\n",
    "\n",
    "print(\"üìÅ Current Core ML Models:\")\n",
    "if model_dir.exists():\n",
    "    for model_file in sorted(model_dir.glob(\"*.ml*\")):\n",
    "        if model_file.is_dir():\n",
    "            size = sum(f.stat().st_size for f in model_file.rglob('*') if f.is_file())\n",
    "        else:\n",
    "            size = model_file.stat().st_size\n",
    "        size_mb = size / (1024 * 1024)\n",
    "        print(f\"  {model_file.name:<40} {size_mb:>8.1f} MB\")\nelse:\n    print(\"‚ùå Model directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create .mlprogram Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .mlprogram versions for comparison\n",
    "print(\"üîß Creating .mlprogram models...\")\n",
    "\n",
    "try:\n",
    "    create_mlprogram_models(\n",
    "        input_model_path=\"../coreml_models/franca_vitb14_in21k_fp32.mlpackage\",\n",
    "        output_dir=\"../coreml_models\"\n",
    "    )\n",
    "    print(\"‚úÖ .mlprogram creation complete!\")\nexcept Exception as e:\n    print(f\"‚ùå Error creating .mlprogram models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize comparator and load all available models\n",
    "comparator = ModelFormatComparator()\n",
    "comparator.load_models(\"../coreml_models\")\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(comparator.models)} model formats for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance benchmarks\n",
    "print(\"‚ö° Running performance benchmarks...\")\n",
    "comparator.benchmark_performance(test_image_path=\"../assets/dog.jpg\", num_runs=15)\n",
    "\n",
    "# Display results\n",
    "if comparator.results:\n",
    "    print(\"\\nüìä Performance Results:\")\n",
    "    for format_name, result in comparator.results.items():\n",
    "        mean_ms = result['mean_time'] * 1000\n",
    "        std_ms = result['std_time'] * 1000\n",
    "        print(f\"  {format_name:20}: {mean_ms:6.1f} ¬± {std_ms:4.1f} ms ({result['size_mb']:5.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accuracy between formats\n",
    "comparator.compare_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "if len(comparator.results) >= 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    formats = list(comparator.results.keys())\n",
    "    sizes = [comparator.results[f]['size_mb'] for f in formats]\n",
    "    times = [comparator.results[f]['mean_time'] * 1000 for f in formats]\n",
    "    time_stds = [comparator.results[f]['std_time'] * 1000 for f in formats]\n",
    "    \n",
    "    # 1. Model Size Comparison\n",
    "    bars1 = axes[0, 0].bar(formats, sizes, alpha=0.7)\n",
    "    axes[0, 0].set_title('Model Size Comparison')\n",
    "    axes[0, 0].set_ylabel('Size (MB)')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, size in zip(bars1, sizes):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                       f'{size:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Performance Comparison\n",
    "    bars2 = axes[0, 1].bar(formats, times, yerr=time_stds, alpha=0.7, capsize=5)\n",
    "    axes[0, 1].set_title('Inference Time Comparison')\n",
    "    axes[0, 1].set_ylabel('Time (ms)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, time_val in zip(bars2, times):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(time_stds),\n",
    "                       f'{time_val:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Size vs Performance Scatter\n",
    "    colors = sns.color_palette(\"husl\", len(formats))\n",
    "    for i, (fmt, size, time_val) in enumerate(zip(formats, sizes, times)):\n",
    "        axes[1, 0].scatter(size, time_val, s=100, c=[colors[i]], label=fmt, alpha=0.7)\n",
    "        axes[1, 0].annotate(fmt, (size, time_val), xytext=(5, 5), \n",
    "                           textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Model Size (MB)')\n",
    "    axes[1, 0].set_ylabel('Inference Time (ms)')\n",
    "    axes[1, 0].set_title('Size vs Performance Trade-off')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Efficiency Score (Performance per MB)\n",
    "    slowest_time = max(times)\n",
    "    speedups = [slowest_time / t for t in times]\n",
    "    efficiencies = [speedup / size for speedup, size in zip(speedups, sizes)]\n",
    "    \n",
    "    bars4 = axes[1, 1].bar(formats, efficiencies, alpha=0.7)\n",
    "    axes[1, 1].set_title('Efficiency Score (Performance/MB)')\n",
    "    axes[1, 1].set_ylabel('Efficiency Score')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, eff in zip(bars4, efficiencies):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                       f'{eff:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Need at least 2 models for visual comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Vector Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature vectors between different formats\n",
    "if len(comparator.results) >= 2:\n",
    "    print(\"üß† Feature Vector Analysis:\")\n",
    "    \n",
    "    # Get feature vectors\n",
    "    features = {}\n",
    "    for format_name, result in comparator.results.items():\n",
    "        features[format_name] = result['last_output'].flatten()\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Feature distributions\n",
    "    for format_name, feature_vec in features.items():\n",
    "        axes[0, 0].hist(feature_vec, bins=50, alpha=0.6, label=format_name, density=True)\n",
    "    axes[0, 0].set_title('Feature Value Distributions')\n",
    "    axes[0, 0].set_xlabel('Feature Value')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Feature norms\n",
    "    format_names = list(features.keys())\n",
    "    norms = [np.linalg.norm(features[f]) for f in format_names]\n",
    "    axes[0, 1].bar(format_names, norms, alpha=0.7)\n",
    "    axes[0, 1].set_title('Feature Vector Norms')\n",
    "    axes[0, 1].set_ylabel('L2 Norm')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (name, norm) in enumerate(zip(format_names, norms)):\n",
    "        axes[0, 1].text(i, norm + 0.1, f'{norm:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Pairwise correlations (if we have multiple formats)\n",
    "    if len(features) >= 2:\n",
    "        # Create correlation matrix\n",
    "        corr_matrix = np.zeros((len(format_names), len(format_names)))\n",
    "        for i, fmt1 in enumerate(format_names):\n",
    "            for j, fmt2 in enumerate(format_names):\n",
    "                corr = np.corrcoef(features[fmt1], features[fmt2])[0, 1]\n",
    "                corr_matrix[i, j] = corr\n",
    "        \n",
    "        im = axes[1, 0].imshow(corr_matrix, cmap='RdYlBu_r', vmin=0.9, vmax=1.0)\n",
    "        axes[1, 0].set_xticks(range(len(format_names)))\n",
    "        axes[1, 0].set_yticks(range(len(format_names)))\n",
    "        axes[1, 0].set_xticklabels(format_names, rotation=45)\n",
    "        axes[1, 0].set_yticklabels(format_names)\n",
    "        axes[1, 0].set_title('Feature Correlation Matrix')\n",
    "        \n",
    "        # Add correlation values\n",
    "        for i in range(len(format_names)):\n",
    "            for j in range(len(format_names)):\n",
    "                axes[1, 0].text(j, i, f'{corr_matrix[i, j]:.4f}', \n",
    "                               ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        plt.colorbar(im, ax=axes[1, 0])\n",
    "    \n",
    "    # 4. Feature differences (relative to first format)\n",
    "    if len(features) >= 2:\n",
    "        reference_name = format_names[0]\n",
    "        reference_features = features[reference_name]\n",
    "        \n",
    "        for format_name in format_names[1:]:\n",
    "            diff = features[format_name] - reference_features\n",
    "            axes[1, 1].hist(diff, bins=50, alpha=0.6, \n",
    "                           label=f'{format_name} - {reference_name}', density=True)\n",
    "        \n",
    "        axes[1, 1].set_title(f'Feature Differences (vs {reference_name})')\n",
    "        axes[1, 1].set_xlabel('Difference')\n",
    "        axes[1, 1].set_ylabel('Density')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical comparison\n",
    "    print(f\"\\nüìà Feature Statistics:\")\n",
    "    for format_name, feature_vec in features.items():\n",
    "        print(f\"  {format_name:20}: norm={np.linalg.norm(feature_vec):8.4f}, \"\n",
    "              f\"mean={feature_vec.mean():8.4f}, std={feature_vec.std():8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "comparator.generate_summary()\n",
    "\n",
    "# Additional insights\n",
    "if comparator.results:\n",
    "    print(f\"\\nüéØ Key Insights:\")\n",
    "    \n",
    "    # FP16 vs FP32 analysis\n",
    "    fp32_results = {k: v for k, v in comparator.results.items() if 'fp32' in k}\n",
    "    fp16_results = {k: v for k, v in comparator.results.items() if 'fp16' in k}\n",
    "    \n",
    "    if fp32_results and fp16_results:\n",
    "        fp32_key = list(fp32_results.keys())[0]\n",
    "        fp16_key = list(fp16_results.keys())[0]\n",
    "        \n",
    "        size_reduction = (1 - fp16_results[fp16_key]['size_mb'] / fp32_results[fp32_key]['size_mb']) * 100\n",
    "        time_change = (fp16_results[fp16_key]['mean_time'] / fp32_results[fp32_key]['mean_time'] - 1) * 100\n",
    "        \n",
    "        print(f\"  üì± FP16 vs FP32: {size_reduction:.1f}% size reduction, {time_change:+.1f}% time change\")\n",
    "    \n",
    "    # .mlpackage vs .mlprogram analysis\n",
    "    mlpackage_results = {k: v for k, v in comparator.results.items() if 'mlpackage' in k or 'fp' in k}\n",
    "    mlprogram_results = {k: v for k, v in comparator.results.items() if 'mlprogram' in k}\n",
    "    \n",
    "    if mlprogram_results:\n",
    "        print(f\"  üì¶ .mlprogram format: Available for deployment and distribution\")\n",
    "        print(f\"  üîß .mlpackage format: Better for development and debugging\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Deployment Recommendations:\")\n",
    "    print(f\"  ‚Ä¢ iOS/macOS Apps: Use FP16 .mlprogram for best size/performance balance\")\n",
    "    print(f\"  ‚Ä¢ Development: Use FP32 .mlpackage for debugging and validation\")\n",
    "    print(f\"  ‚Ä¢ Production: Consider FP16 if accuracy loss is acceptable\")\n",
    "    print(f\"  ‚Ä¢ Distribution: .mlprogram format is more portable\")"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.10.8"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}
